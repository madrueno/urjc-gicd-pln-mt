{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 6: RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio A1 - Pipeline RAG y puntos de fallo\n",
    "\n",
    "Explica el pipeline RAG completo (ingestión → chunking → embeddings → vector store → retrieval → prompting → generación) e identifica al menos 4 puntos donde el sistema puede fallar (p.ej., chunking mal calibrado, retrieval irrelevante, etc.). Para cada punto de fallo, propone una mitigación concreta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestión de documentos → limpieza → chunking → embeddings → indexado en vector store → retrieval (top-k/MMR) → construcción de prompt con contexto → generación → post-procesado (citas/guardrails)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puntos de fallo típicos y mitigaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Punto de fallo | Mitigación |\n",
    "|---|---|---|\n",
    "| 1 | **Chunking demasiado grande/pequeño** | Ajustar `chunk_size`/`overlap`. Usar splitters por estructura (párrafos, secciones). |\n",
    "| 2 | **Embeddings inadecuados** (dominio/idioma) | Usar embeddings específicos de dominio o multilingües; normalización de vectores. |\n",
    "| 3 | **Retrieval con ruido** | MMR para diversificar, filtros por metadatos, re-ranking, query rewriting. |\n",
    "| 4 | **Context window saturada** | Compresión de contexto, top-k adaptativo, reducción por scoring. |\n",
    "| 5 | **Generación no fiel** (alucinaciones) | Prompt con \"use only context\", forzar citas; verificación de groundedness. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio A2 - RAG vs Fine-Tuning vs Prompting\n",
    "\n",
    "Para un asistente sobre normativa universitaria interna (documentos privados que cambian cada semestre): (1) ¿Cuándo elegirías RAG? (2) ¿Cuándo elegirías fine-tuning? (3) ¿Qué papel juega el prompt engineering en ambos? Da una respuesta razonada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) ¿Cuándo RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG es preferible cuando el conocimiento es **privado y cambia con frecuencia** y necesitamos **trazabilidad** (saber de qué documento viene la respuesta). En este caso, la normativa cambia cada semestre, por lo que RAG permite actualizar el corpus sin reentrenar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) ¿Cuándo Fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning solo si el estilo o la tarea es **estable** y hay datos de alta calidad para entrenar (y no cambia cada semestre). Por ejemplo, si se quisiera que el modelo adoptase un tono formal específico de la universidad de forma permanente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Papel del Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting es **complementario** a ambos enfoques: define el formato de respuesta, fuerza la inclusión de citas, controla la abstención cuando no hay información suficiente y reduce alucinaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio A3 - Evaluación de RAG\n",
    "\n",
    "Propón un plan de evaluación con al menos 3 métricas o criterios: (i) métricas de retrieval (p.ej., Recall@k), (ii) métricas de respuesta (p.ej., exactitud/fidelidad), (iii) métricas de robustez (p.ej., sensibilidad a perturbaciones). Indica cómo obtendrías datos de evaluación (conjunto de preguntas, gold answers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Categoría | Métricas | Descripción |\n",
    "|---|---|---|\n",
    "| **(i) Retrieval** | Recall@k, MRR | Medir sobre preguntas con documentos gold si los chunks relevantes aparecen en el top-k recuperado. |\n",
    "| **(ii) Respuesta** | Exactitud, Faithfulness, EM/F1 | Evaluar si la respuesta es correcta y si está **apoyada en el contexto** recuperado (no alucinada). EM/F1 si hay respuestas canónicas. |\n",
    "| **(iii) Robustez** | Sensibilidad a perturbaciones | Perturbaciones de la pregunta (paráfrasis), cambios de k, introducción de ruido en el corpus. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de datos de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear un banco de **30–50 preguntas**.\n",
    "- Para cada pregunta: documento(s) relevante(s) y respuesta esperada.\n",
    "- **Evaluación humana** en casos ambiguos donde las métricas automáticas no sean suficientes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
