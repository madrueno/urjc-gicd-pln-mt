{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tema 4: Redes neuronales para clasificación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjamnpb4yi7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_MODELS = Path.cwd().parent / 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fb38b",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Entrenar un modelo simple de red neuronal (MLP) utilizando Keras de TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34a6d8",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Estoy un poco harto del día a día , nada mejora',\n",
    "             'Hoy es un buen día',\n",
    "             'No se te ve satisfecho con el trabajo',\n",
    "             'Este paisaje es hermoso y bonito']\n",
    "\n",
    "# 1: positivo, 0: negativo\n",
    "labels = [0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0f4f9",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Funciones de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6054132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filtered(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) < 4)\n",
    "\n",
    "def spacy_processing(doc, filtering, lematization):\n",
    "    tokens = []\n",
    "    if filtering and lematization:\n",
    "        tokens = [token.lemma_ for token in doc if token_filtered(token)]\n",
    "    elif lematization:\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "    elif filtering:\n",
    "        tokens = [token.text for token in doc if token_filtered(token)]\n",
    "    else:\n",
    "        tokens = [token.text for token in doc]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f1f3",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Preparar datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def prepare_vocabulary(corpus, vocab_size, normalize):\n",
    "    token_to_index = {}\n",
    "    current_index = 1\n",
    "\n",
    "    for sentence in corpus:\n",
    "        doc = nlp(sentence)\n",
    "        if normalize:\n",
    "            doc = nlp(spacy_processing(doc, True, True))\n",
    "        for token in doc:\n",
    "            if token.text not in token_to_index and current_index < vocab_size:\n",
    "                token_to_index[token.text] = current_index\n",
    "                current_index += 1\n",
    "\n",
    "    return token_to_index\n",
    "\n",
    "\n",
    "def prepare_sentences(corpus, vocabulary, max_length, normalize):\n",
    "    encoded_sentences = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        doc = nlp(sentence)\n",
    "        if normalize:\n",
    "            doc = nlp(spacy_processing(doc, True, True))\n",
    "        encoded_sentence = []\n",
    "        for token in doc:\n",
    "            if token.text in vocabulary:\n",
    "                encoded_sentence.append(vocabulary[token.text])\n",
    "            else:\n",
    "                encoded_sentence.append(0)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    prepared_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post', truncating='post')\n",
    "    print(\"Oraciones originales(\", len(corpus), \"):\")\n",
    "    print(corpus)\n",
    "    print(\"Oraciones procesadas(\", len(prepared_sentences), \"):\")\n",
    "    print(prepared_sentences)\n",
    "    return prepared_sentences\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "max_length = 10\n",
    "\n",
    "vocabulary_train = prepare_vocabulary(sentences, vocab_size, True)\n",
    "print(\"\\nVocabulario (\", len(vocabulary_train), \"):\")\n",
    "print(vocabulary_train)\n",
    "prepared_sentences = prepare_sentences(sentences, vocabulary_train, max_length, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984662a",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Configurar el modelo MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "vector_size = 8\n",
    "model.add(Embedding(vocab_size, vector_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(\"Red diseñada correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4694b4",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Compilar y entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64851cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501adcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "prepared_sentences = np.array(prepared_sentences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6c6db",
   "metadata": {},
   "source": [
    "### Apartado f\n",
    "Evaluar el modelo con nuevas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"No fui al estreno de la película porque nadie me quería acompañar\",\n",
    "    \"Envidio de buena manera a los que tienen la oportunidad de ir mañana al estadio\",\n",
    "    \"Se nos está volviendo costumbre del domingo por la noche, ver el episodio anterior de SNL y eso me hace recibir el lunes con mejor humor\",\n",
    "    \"Al final decidí no ir al cine porque estaba cansada\",\n",
    "    \"Todo es maravilloso y formidable, muy bonito\"\n",
    "]\n",
    "\n",
    "print(\"\\nVocabulario (\", len(vocabulary_train), \"):\")\n",
    "print(vocabulary_train)\n",
    "\n",
    "prepared_test = prepare_sentences(test_sentences, vocabulary_train, max_length, True)\n",
    "\n",
    "predictions = model.predict(prepared_test)\n",
    "\n",
    "print(\"Predicciones detalladas:\")\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    pred = predictions[i][0]\n",
    "    sentiment = \"Positivo\" if pred > 0.5 else \"Negativo\"\n",
    "    print(f\"\\nTexto: {sentence}\")\n",
    "    print(f\"Predicción numérica: {pred:.4f}\")\n",
    "    print(f\"Sentimiento predicho: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Ejercicio 2 - RNN\n",
    "Entrenar RNNs (SimpleRNN) utilizando Keras de TensorFlow para detectar noticias falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Importar librerías y cargar modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'\n",
    "PATH_MODELS = Path.cwd().parent / 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "w2v = KeyedVectors.load_word2vec_format(str(PATH_MODELS / 'SBW-vectors-300-min5.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Cargar y explorar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(str(PATH_DATA / 'train.xlsx'), engine=\"openpyxl\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Análisis rápido de longitud de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Headline']\n",
    " .apply(lambda x: len(x.split()))\n",
    " .describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Text']\n",
    " .apply(lambda x: len(x.split()))\n",
    " .describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, headlines = df['Text'].tolist(), df['Headline'].tolist()\n",
    "labels = np.array([1 if cat == 'Fake' else 0 for cat in df['Category']])\n",
    "\n",
    "len(headlines), len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Funciones de indexación y transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexer(corpus, vocab_size):\n",
    "    token_to_index = {}\n",
    "    current_index = 1\n",
    "\n",
    "    for sentence in tqdm(corpus):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            clean_token = token.text.lower()\n",
    "            if clean_token not in token_to_index and current_index < vocab_size:\n",
    "                token_to_index[clean_token] = current_index\n",
    "                current_index += 1\n",
    "\n",
    "    print(\"\\nVocabulario (\", len(token_to_index), \"):\")\n",
    "    print(token_to_index)\n",
    "\n",
    "    return token_to_index\n",
    "\n",
    "\n",
    "def transform_text(corpus, token_to_index, max_length=10):\n",
    "    encoded_sentences = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        doc = nlp(sentence)\n",
    "        encoded_sentence = []\n",
    "        for token in doc:\n",
    "            clean_token = token.text.lower()\n",
    "            if clean_token in token_to_index:\n",
    "                encoded_sentence.append(token_to_index[clean_token])\n",
    "            else:\n",
    "                encoded_sentence.append(0)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    prepared_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post', truncating='post')\n",
    "    print(\"Oraciones procesadas(\", len(prepared_sentences), \"):\")\n",
    "    print(prepared_sentences)\n",
    "\n",
    "    return prepared_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(w2v_model, vocab, emb_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, emb_dim))\n",
    "    for token, idx in vocab.items():\n",
    "        if token in w2v_model:\n",
    "            embedding_matrix[idx] = w2v_model[token]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim,))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Preparar datos: separación train/validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_headlines = 5000\n",
    "\n",
    "X_train_headlines, X_val_headlines, y_train_headlines, y_val_headlines = train_test_split(headlines, labels, test_size=0.3, random_state=42)\n",
    "X_val_headlines, X_test_headlines, y_val_headlines, y_test_headlines = train_test_split(X_val_headlines, y_val_headlines, test_size=0.5, random_state=42)\n",
    "\n",
    "token_to_index_headlines = build_indexer(X_train_headlines, vocab_size=vocab_size_headlines)\n",
    "\n",
    "print(len(X_train_headlines), len(X_val_headlines), len(X_test_headlines))\n",
    "print(len(y_train_headlines), len(y_val_headlines), len(y_test_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_texts = 20000\n",
    "\n",
    "X_train_texts, X_val_texts, y_train_texts, y_val_texts = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "X_val_texts, X_test_texts, y_val_texts, y_test_texts = train_test_split(X_val_texts, y_val_texts, test_size=0.5, random_state=42)\n",
    "\n",
    "token_to_index_texts = build_indexer(X_train_texts, vocab_size=vocab_size_texts)\n",
    "\n",
    "print(len(X_train_texts), len(X_val_texts), len(X_test_texts))\n",
    "print(len(y_train_texts), len(y_val_texts), len(y_test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Apartado f\n",
    "Obtener matrices de índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_headlines = 15\n",
    "\n",
    "X_train_headlines = transform_text(X_train_headlines, token_to_index_headlines, max_length_headlines)\n",
    "X_val_headlines = transform_text(X_val_headlines, token_to_index_headlines, max_length_headlines)\n",
    "X_test_headlines = transform_text(X_test_headlines, token_to_index_headlines, max_length_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_texts = 500\n",
    "\n",
    "X_train_texts = transform_text(X_train_texts, token_to_index_texts, max_length_texts)\n",
    "X_val_texts = transform_text(X_val_texts, token_to_index_texts, max_length_texts)\n",
    "X_test_texts = transform_text(X_test_texts, token_to_index_texts, max_length_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Apartado g\n",
    "Entrenar RNN con headlines (embeddings desde cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "vector_size = 128\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size_headlines, output_dim=vector_size))\n",
    "model.add(SimpleRNN(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_headlines))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_headlines,\n",
    "    y_train_headlines,\n",
    "    validation_data=(X_val_headlines, y_val_headlines),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_headlines = model.predict(X_test_headlines)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Apartado h\n",
    "Entrenar RNN con headlines (inicialización con Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding_matrix(w2v, token_to_index_headlines, 300)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model.add(SimpleRNN(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_headlines))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_headlines,\n",
    "    y_train_headlines,\n",
    "    validation_data=(X_val_headlines, y_val_headlines),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_headlines = model.predict(X_test_headlines)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Apartado i\n",
    "Entrenar RNN con textos (embeddings desde cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "vector_size = 128\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size_texts, output_dim=vector_size))\n",
    "model.add(SimpleRNN(8, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_texts))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts,\n",
    "    y_train_texts,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_texts = model.predict(X_test_texts)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Apartado j\n",
    "Entrenar RNN con textos (inicialización con Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding_matrix(w2v, token_to_index_texts, 300)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model.add(SimpleRNN(8, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_texts))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts,\n",
    "    y_train_texts,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_texts = model.predict(X_test_texts)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Ejercicio 3 - LSTM\n",
    "Entrenar LSTMs utilizando Keras de TensorFlow para detectar noticias falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Entrenar LSTM con headlines (embeddings desde cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "vector_size = 128\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size_headlines, output_dim=vector_size))\n",
    "model.add(LSTM(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_headlines))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_headlines,\n",
    "    y_train_headlines,\n",
    "    validation_data=(X_val_headlines, y_val_headlines),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_headlines = model.predict(X_test_headlines)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Entrenar LSTM con headlines (inicialización con Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding_matrix(w2v, token_to_index_headlines, 300)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model.add(LSTM(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_headlines))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_headlines,\n",
    "    y_train_headlines,\n",
    "    validation_data=(X_val_headlines, y_val_headlines),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_headlines = model.predict(X_test_headlines)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Entrenar LSTM con textos (embeddings desde cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "vector_size = 128\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size_texts, output_dim=vector_size))\n",
    "model.add(LSTM(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_texts))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts,\n",
    "    y_train_texts,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_texts = model.predict(X_test_texts)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Entrenar LSTM con textos (inicialización con Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_embedding_matrix(w2v, token_to_index_texts, 300)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model.add(LSTM(8, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length_texts))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts,\n",
    "    y_train_texts,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlyStopping]\n",
    ")\n",
    "\n",
    "y_pred_texts = model.predict(X_test_texts)\n",
    "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "04-redes-neuronales (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
