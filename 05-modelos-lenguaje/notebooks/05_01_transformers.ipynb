{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 5: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wumseg1z58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e881641",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Entrenar un encoder transformer simplificado para análisis de sentimientos en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385a379",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Importar librerías y definir datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09920fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, MultiHeadAttention, LayerNormalization, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'Estoy un poco harto del día a día, nada mejora',\n",
    "    'Hoy es un buen día',\n",
    "    'No se te ve satisfecho con el trabajo',\n",
    "    'Este paisaje es hermoso y bonito'\n",
    "]\n",
    "labels = [0, 1, 0, 1]  # 1: Positivo, 0: Negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344881f",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Preprocesamiento del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 10\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0078930",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Crear modelo transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_classifier(vocab_size, max_length):\n",
    "    inputs = Input(shape=(max_length,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size, 32)(inputs)\n",
    "\n",
    "    attention = MultiHeadAttention(num_heads=2, key_dim=32)(embedding_layer, embedding_layer, embedding_layer)\n",
    "\n",
    "    x = LayerNormalization()(attention + embedding_layer)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = create_transformer_classifier(vocab_size, max_length)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db211b",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, labels,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47d5a6",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Evaluar con nuevas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'No fui al estreno de la película porque nadie me quería acompañar',\n",
    "    'Envidio de buena manera a los que tienen la oportunidad de ir mañana al estadio',\n",
    "    'Se nos está volviendo costumbre del domingo por la noche, ver el episodio anterior de SNL y eso me hace recibir el lunes con mejor humor',\n",
    "    'Al final decidí no ir al cine porque estaba cansada'\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length)\n",
    "test_padded = tf.convert_to_tensor(test_padded, dtype=tf.float32)\n",
    "\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "for sentence, prediction in zip(test_sentences, predictions):\n",
    "    sentiment = \"Positivo\" if prediction > 0.5 else \"Negativo\"\n",
    "    print(f\"Frase: '{sentence}'\")\n",
    "    print(f\"Sentimiento: {sentiment} (probabilidad: {prediction[0]:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5021238",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "Crear una arquitectura transformer tipo encoder para detectar fake news en español. Comparar versiones con y sin positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras_nlp.layers import PositionEmbedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "df = pd.read_excel(str(PATH_DATA / 'train.xlsx'), engine=\"openpyxl\")\n",
    "df.head()\n",
    "\n",
    "texts = df[\"Text\"].astype(str).tolist()\n",
    "labels = df[\"Category\"].astype(str).tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c9a29",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "División del dataset y tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(texts, labels, test_size=0.2, random_state=SEED)\n",
    "X_val_texts, X_test_texts, y_val, y_test = train_test_split(X_temp_texts, y_temp, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303bd741",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Definir bloque transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    return LayerNormalization(epsilon=1e-6)(x + ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3de7f",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Construir modelo parametrizado (con y sin positional embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(use_positional_embedding):\n",
    "    inputs = Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "\n",
    "    x = Embedding(input_dim=VOCAB_SIZE, output_dim=64)(inputs)\n",
    "\n",
    "    if use_positional_embedding:\n",
    "        pos_x = PositionEmbedding(sequence_length=MAX_LEN)(x)\n",
    "        x = x + pos_x\n",
    "\n",
    "    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfdf69",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Entrenar y evaluar ambos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97095c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(use_positional_embedding):\n",
    "    name = \"CON_POSITIONAL\" if use_positional_embedding else \"SIN_POSITIONAL\"\n",
    "    print(f\"\\n Entrenando modelo: {name}\\n\")\n",
    "\n",
    "    model = build_model(use_positional_embedding)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, verbose=2)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "    print(f\"\\n Reporte para {name}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b79854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_pos = train_and_evaluate(use_positional_embedding=True)\n",
    "model_without_pos = train_and_evaluate(use_positional_embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191f9e7",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "Utilizar un decoder transformer para predicción autorregresiva de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47236ceb",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Importar librerías y definir datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb054a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    \"El perro corre feliz\",\n",
    "    \"El gato salta ágil\",\n",
    "    \"La tortuga camina lenta\",\n",
    "    \"El caballo trota fuerte\",\n",
    "    \"El perro ladra ruidoso\",\n",
    "    \"El gato duerme tranquilo\",\n",
    "    \"La tortuga nada lenta\",\n",
    "    \"El caballo galopa veloz\",\n",
    "    \"El perro juega contento\",\n",
    "    \"El gato observa curioso\",\n",
    "    \"La tortuga descansa pacífica\",\n",
    "    \"El caballo relincha bravo\",\n",
    "    \"El perro huele atento\",\n",
    "    \"El gato maúlla suave\",\n",
    "    \"La tortuga explora cautelosa\",\n",
    "    \"El caballo corre elegante\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7c4ca",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Preprocesamiento del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 40\n",
    "max_length = 10\n",
    "embedding_dim = 32\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    X.append(seq[:-1])\n",
    "    y.append(seq[1:])\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_length)\n",
    "y = pad_sequences(y, maxlen=max_length)\n",
    "\n",
    "X = tf.convert_to_tensor(X, dtype=tf.int32)\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213ad1a",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Crear modelo transformer decoder con causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc94a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_decoder(vocab_size, max_length):\n",
    "    inputs = Input(shape=(max_length,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "    attention = MultiHeadAttention(num_heads=2, key_dim=embedding_dim)(embedding_layer, embedding_layer, embedding_layer, use_causal_mask=True)\n",
    "\n",
    "    x = LayerNormalization()(attention + embedding_layer)\n",
    "\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = create_transformer_decoder(vocab_size, max_length)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780c3ab",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=10,\n",
    "    batch_size=2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b425df",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Predicción de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, tokenizer, sentence, max_length):\n",
    "    seq = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    seq = pad_sequences([seq], maxlen=max_length)\n",
    "\n",
    "    predictions = model.predict(seq)\n",
    "\n",
    "    last_pred_logits = predictions[0, -1]\n",
    "    next_token_id = np.argmax(last_pred_logits)\n",
    "\n",
    "    return tokenizer.index_word.get(next_token_id, None)\n",
    "\n",
    "\n",
    "sentence = \"El caballo\"\n",
    "next_token = predict_next_token(model, tokenizer, sentence, max_length)\n",
    "print(f\"Sentence: '{sentence}' --> Siguiente token predicho: '{next_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_tokens(model, tokenizer, sentence, max_length):\n",
    "    next_token = predict_next_token(model, tokenizer, sentence, max_length)\n",
    "\n",
    "    predicted_tokens = [next_token]\n",
    "    while next_token is not None:\n",
    "        sentence += \" \" + next_token\n",
    "        next_token = predict_next_token(model, tokenizer, sentence, max_length)\n",
    "        predicted_tokens.append(next_token)\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "\n",
    "sentence = \"El caballo\"\n",
    "predicted_tokens = predict_all_tokens(model, tokenizer, sentence, max_length)\n",
    "print(f\"Sentence: '{sentence}' --> Tokens predichos: {predicted_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8e567",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "Crear una arquitectura completa de transformers (encoder y decoder) para traducción del inglés al español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30b90b",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Importar librerías y definir datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "sentences_english = [\n",
    "    \"hello world\",\n",
    "    \"goodbye world\",\n",
    "    \"hello everyone\",\n",
    "]\n",
    "\n",
    "sentences_spanish = [\n",
    "    \"hola mundo\",\n",
    "    \"adiós mundo\",\n",
    "    \"hola a todos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebd5e5",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Preparación de datos y tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "sentences_spanish = [f\"<bos> {sent} <eos>\" for sent in sentences_spanish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af192fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "encoder_tokenizer.fit_on_texts(sentences_english)\n",
    "encoder_seq = encoder_tokenizer.texts_to_sequences(sentences_english)\n",
    "\n",
    "encoder_vocab_size = len(encoder_tokenizer.word_index) + 1\n",
    "encoder_max_len = max(len(seq) for seq in encoder_seq)\n",
    "\n",
    "encoder_inputs = pad_sequences(encoder_seq, maxlen=encoder_max_len, padding='post')\n",
    "\n",
    "decoder_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "decoder_tokenizer.fit_on_texts(sentences_spanish)\n",
    "decoder_seq = decoder_tokenizer.texts_to_sequences(sentences_spanish)\n",
    "\n",
    "decoder_vocab_size = len(decoder_tokenizer.word_index) + 1\n",
    "decoder_max_len = max(len(seq) for seq in decoder_seq)\n",
    "\n",
    "decoder_all = pad_sequences(decoder_seq, maxlen=decoder_max_len, padding='post')\n",
    "\n",
    "decoder_inputs = decoder_all[:, :-1]\n",
    "decoder_targets = decoder_all[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127e1a2",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Definir bloques transformer encoder y decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(query=inputs, value=inputs, key=inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "\n",
    "    return LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "\n",
    "def transformer_decoder(inputs, encoder_output, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(query=inputs, value=inputs, key=inputs, use_causal_mask=True)\n",
    "    attn1 = Dropout(dropout)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn1)\n",
    "\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(query=out1, value=encoder_output, key=encoder_output)\n",
    "    attn2 = Dropout(dropout)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    ff = Dense(ff_dim, activation='relu')(out2)\n",
    "    ff = Dense(out2.shape[-1])(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "\n",
    "    return LayerNormalization(epsilon=1e-6)(out2 + ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991055bc",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Construir modelo encoder-decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "head_size = 64\n",
    "num_heads = 2\n",
    "ff_dim = 128\n",
    "\n",
    "en_in = Input(shape=(None,), dtype=\"int32\")\n",
    "dec_in = Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "enc_embed = Embedding(encoder_vocab_size, embedding_dim)(en_in)\n",
    "enc_out = transformer_encoder(enc_embed, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim)\n",
    "\n",
    "dec_embed = Embedding(decoder_vocab_size, embedding_dim)(dec_in)\n",
    "dec_out = transformer_decoder(dec_embed, enc_out, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim)\n",
    "\n",
    "outputs = Dense(decoder_vocab_size)(dec_out)\n",
    "model = Model([en_in, dec_in], outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679966fa",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ea3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_inputs, decoder_inputs], decoder_targets, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2dfa6",
   "metadata": {},
   "source": [
    "### Apartado f\n",
    "Traducir nuevas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b673c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = decoder_tokenizer.word_index['<eos>']\n",
    "bos_token = decoder_tokenizer.word_index['<bos>']\n",
    "\n",
    "\n",
    "def translate_sentence(input_sentence):\n",
    "    encoder_input = encoder_tokenizer.texts_to_sequences([input_sentence])\n",
    "    encoder_input = pad_sequences(encoder_input, maxlen=encoder_max_len, padding='post')\n",
    "\n",
    "    decoder_output = [bos_token]\n",
    "\n",
    "    for _ in range(decoder_max_len - 1):\n",
    "        decoder_input = pad_sequences([decoder_output], maxlen=decoder_max_len-1, padding='post')\n",
    "\n",
    "        pred = model.predict([encoder_input, decoder_input], verbose=0)\n",
    "        next_token = np.argmax(pred[0, len(decoder_output)-1])\n",
    "\n",
    "        decoder_output.append(next_token)\n",
    "        if next_token == eos_token:\n",
    "            break\n",
    "\n",
    "    output_tokens = [\n",
    "        decoder_tokenizer.index_word[token]\n",
    "        for token in decoder_output\n",
    "        if token != eos_token and token != bos_token\n",
    "    ]\n",
    "    return ' '.join(output_tokens)\n",
    "\n",
    "\n",
    "print(translate_sentence(\"hello world\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05-transformers (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
