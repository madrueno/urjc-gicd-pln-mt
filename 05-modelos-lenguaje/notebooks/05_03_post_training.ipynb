{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tema 5: Post-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Ejercicio 1 - Entendiendo el papel del post-training\n",
    "\n",
    "Un LLM base responde así:\n",
    "\n",
    "**Pregunta:**\n",
    "\"¿Puedo usar antibióticos para tratar un resfriado común?\"\n",
    "\n",
    "**Respuesta del modelo base:**\n",
    "\"Los antibióticos matan bacterias. Algunos médicos los recetan en infecciones respiratorias.\"\n",
    "\n",
    "a) Explica por qué esta respuesta puede ser problemática desde el punto de vista de alineación.\n",
    "\n",
    "b) ¿Qué objetivo del post-training está relacionado con este problema?\n",
    "\n",
    "c) ¿Qué técnica de post-training ayudaría a corregir este tipo de comportamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Problema de alineación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "La respuesta no es claramente incorrecta, pero puede inducir a un uso inapropiado de antibióticos. No deja claro que los antibióticos **no sirven para infecciones virales** como el resfriado común, lo que puede generar un comportamiento perjudicial. Es un ejemplo de una respuesta plausible, **pero potencialmente dañina**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Objetivo de post-training relacionado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "Este caso se relaciona con:\n",
    "\n",
    "- **Seguridad y responsabilidad**\n",
    "- **Reducción de información potencialmente peligrosa**\n",
    "- **Alineación con buenas prácticas médicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Técnica que ayudaría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "RLHF o DPO con ejemplos donde humanos prefieran respuestas que:\n",
    "\n",
    "- Incluyan advertencias\n",
    "- Eviten recomendaciones médicas incorrectas\n",
    "- Añadan matices de seguridad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Ejercicio 2 - SFT vs RLHF\n",
    "\n",
    "Se quiere mejorar un modelo que ya responde correctamente a preguntas de historia, pero sus respuestas son largas, poco claras y a veces demasiado técnicas.\n",
    "\n",
    "a) ¿Sería suficiente aplicar SFT? Justifica.\n",
    "\n",
    "b) ¿Qué aportaría RLHF que SFT no puede capturar fácilmente?\n",
    "\n",
    "c) Da un ejemplo concreto de preferencia humana que RLHF sí podría aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "¿Basta SFT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "Sí podría ayudar parcialmente, ya que SFT con ejemplos bien redactados puede enseñar:\n",
    "\n",
    "- Respuestas más claras\n",
    "- Mejor estructura\n",
    "\n",
    "Pero puede no capturar bien preferencias más sutiles como nivel de detalle adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Qué aporta RLHF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "RLHF permite aprender comparaciones del tipo:\n",
    "\n",
    "- \"Esta respuesta es demasiado técnica\"\n",
    "- \"Esta es más útil para un usuario medio\"\n",
    "\n",
    "Es decir, captura **calidad relativa**, no solo corrección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Ejemplo de preferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "Entre dos respuestas correctas, los humanos pueden preferir la que:\n",
    "\n",
    "- Usa ejemplos\n",
    "- Es más breve\n",
    "- Evita jerga técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Ejercicio 3 - Modelado de preferencias\n",
    "\n",
    "Se presentan dos respuestas del modelo a la pregunta: \"Explica qué es el cambio climático.\"\n",
    "\n",
    "**Respuesta A:** Explicación muy técnica, correcta pero difícil de entender.\n",
    "\n",
    "**Respuesta B:** Explicación clara, con ejemplos cotidianos, pero menos detallada.\n",
    "\n",
    "a) ¿Por qué este tipo de comparación es clave en post-training?\n",
    "\n",
    "b) ¿Qué técnica usaría directamente pares A vs B como señal de aprendizaje?\n",
    "\n",
    "c) ¿Qué riesgo aparece si siempre se favorece la respuesta más simple?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Por qué es clave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "Porque el modelo necesita aprender que no todas las respuestas correctas son igualmente buenas. Las comparaciones permiten modelar criterios humanos como claridad o utilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Técnica que usa pares directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "**DPO** (Direct Preference Optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Riesgo de favorecer siempre lo simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "Puede producir:\n",
    "\n",
    "- Pérdida de profundidad\n",
    "- Simplificaciones excesivas\n",
    "- Modelo demasiado superficial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Ejercicio 4 - Pipeline de post-training\n",
    "\n",
    "Ordena las siguientes etapas y justifica brevemente:\n",
    "- Instruction Tuning\n",
    "- Modelado de preferencias\n",
    "- SFT\n",
    "- Optimización por preferencias (RLHF/DPO)\n",
    "- Fine-tuning de dominio\n",
    "\n",
    "Luego responde: ¿Qué aporta cada etapa que no aportan las anteriores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Solución\n",
    "\n",
    "**Orden correcto típico:**\n",
    "\n",
    "1. SFT\n",
    "2. Instruction Tuning\n",
    "3. Modelado de preferencias\n",
    "4. Optimización por preferencias (RLHF/DPO)\n",
    "5. Fine-tuning de dominio\n",
    "\n",
    "**Aporte de cada etapa:**\n",
    "\n",
    "| Etapa | Aporte |\n",
    "|-------|--------|\n",
    "| SFT | Enseña a responder bien |\n",
    "| Instruction tuning | Enseña a seguir instrucciones variadas |\n",
    "| Preferencias | Enseña qué respuesta es mejor |\n",
    "| Optimización | Ajusta el comportamiento hacia esas preferencias |\n",
    "| Dominio | Especializa el modelo |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Ejercicio 5 - Riesgos del post-training\n",
    "\n",
    "Explica brevemente (2-3 líneas cada uno):\n",
    "\n",
    "a) Reward hacking\n",
    "\n",
    "b) Sobrealineación\n",
    "\n",
    "c) Pérdida de generalidad tras fine-tuning de dominio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Reward hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "El modelo aprende a maximizar la señal de recompensa sin mejorar realmente la calidad o utilidad de la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Sobrealineación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "El modelo se vuelve demasiado prudente y evita responder incluso cuando sería apropiado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Pérdida de generalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Tras especializarse en un dominio, el modelo puede degradar su rendimiento en tareas generales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Ejercicio 6 - Prompt Engineering vs Post-Training\n",
    "\n",
    "Para el problema: \"El modelo da respuestas demasiado seguras aunque no tenga suficiente información.\"\n",
    "\n",
    "a) ¿Podría mitigarse solo con prompting?\n",
    "\n",
    "b) ¿Qué objetivo de post-training está implicado?\n",
    "\n",
    "c) ¿Qué tipo de señal de entrenamiento ayudaría a que el modelo aprenda a decir \"no lo sé\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "¿Solo prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "Puede ayudar parcialmente (p. ej., \"si no estás seguro, indícalo\"), pero no garantiza que el modelo internalice ese comportamiento de forma consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Objetivo implicado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "Enseñar al modelo a:\n",
    "\n",
    "- Expresar incertidumbre\n",
    "- Abstenerse cuando no tiene información suficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Señal de entrenamiento útil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "Comparaciones donde se prefieran respuestas del tipo:\n",
    "\n",
    "- \"No dispongo de suficiente información\" sobre respuestas inventadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
