{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tema 2: Análisis lingüístico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Análisis con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer, RegexpTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tree import Tree\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Apartado 1.1: Texto en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is the study of the extraction of knowledge from data. It uses various techniques from many fields, including signal processing, mathematics, probability models, machine learning, computer programming, statistics, data engineering, pattern recognition and learning, visualization, uncertainty modeling, data warehousing, and high performance computing with the goal of extracting useful knowledge from the data. Data Science is not restricted to only big data, although the fact that data is scaling up makes big data an important aspect of data science.\n",
      "A practitioner of data science is called a data scientist. Data scientists solve complex data problems using various elements of mathematics, statistics and computer science, although expertise in these subjects are not required. However, a data scientist is most likely to be an expert in only one or two of these disciplines, meaning that cross disciplinary teams can be a key component of data science.\n",
      "Good data scientists are able to apply their skills to achieve a broad spectrum of end results. The skill-sets and competencies that data scientists employ vary widely.\n",
      "Many data scientists practice and showcase their work on platforms such as Kaggle and Google Colab.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(PATH_DATA / 'data_science.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af963ec5",
   "metadata": {},
   "source": [
    "a) Tokenización texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science is the study of the extraction of knowledge from data.',\n",
       " 'It uses various techniques from many fields, including signal processing, mathematics, probability models, machine learning, computer programming, statistics, data engineering, pattern recognition and learning, visualization, uncertainty modeling, data warehousing, and high performance computing with the goal of extracting useful knowledge from the data.',\n",
       " 'Data Science is not restricted to only big data, although the fact that data is scaling up makes big data an important aspect of data science.',\n",
       " 'A practitioner of data science is called a data scientist.',\n",
       " 'Data scientists solve complex data problems using various elements of mathematics, statistics and computer science, although expertise in these subjects are not required.',\n",
       " 'However, a data scientist is most likely to be an expert in only one or two of these disciplines, meaning that cross disciplinary teams can be a key component of data science.',\n",
       " 'Good data scientists are able to apply their skills to achieve a broad spectrum of end results.',\n",
       " 'The skill-sets and competencies that data scientists employ vary widely.',\n",
       " 'Many data scientists practice and showcase their work on platforms such as Kaggle and Google Colab.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "sentences = tokenizer.tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: 215 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'the',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'from',\n",
       " 'data',\n",
       " '.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'various',\n",
       " 'techniques',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk = word_tokenize(text)\n",
    "print(f'word_tokenize: {len(tokens_nltk)} tokens')\n",
    "tokens_nltk[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split(): 189 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'the',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'from',\n",
       " 'data.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'various',\n",
       " 'techniques',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields,',\n",
       " 'including']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_split = text.split()\n",
    "print(f'split(): {len(tokens_split)} tokens')\n",
    "tokens_split[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h0xss9staev",
   "metadata": {},
   "source": [
    "b) Eliminar signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opción 1 (list comprehension): 189 tokens\n",
      "Opción 2 (RegexpTokenizer): 190 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'the',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'from',\n",
       " 'data',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'various',\n",
       " 'techniques',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields',\n",
       " 'including']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opción 1: Filtrar con list comprehension\n",
    "tokens_no_punct = [t for t in tokens_nltk if t not in string.punctuation]\n",
    "print(f'Opción 1 (list comprehension): {len(tokens_no_punct)} tokens')\n",
    "\n",
    "# Opción 2: Usar RegexpTokenizer (tokeniza sin puntuación directamente)\n",
    "tokenizer_words = RegexpTokenizer(r'\\w+')\n",
    "tokens_no_punct_regex = tokenizer_words.tokenize(text)\n",
    "print(f'Opción 2 (RegexpTokenizer): {len(tokens_no_punct_regex)} tokens')\n",
    "\n",
    "tokens_no_punct[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj1hvkngrek",
   "metadata": {},
   "source": [
    "c) Palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con puntuación: [(',', 17), ('data', 16), ('of', 9), ('.', 9), ('and', 6)]\n",
      "Sin puntuación: [('data', 16), ('of', 9), ('and', 6), ('science', 5), ('is', 5)]\n",
      "\"data\" aparece 19 veces\n"
     ]
    }
   ],
   "source": [
    "freq_before = FreqDist(tokens_nltk)\n",
    "print('Con puntuación:', freq_before.most_common(5))\n",
    "\n",
    "freq_after = FreqDist(tokens_no_punct)\n",
    "print('Sin puntuación:', freq_after.most_common(5))\n",
    "\n",
    "freq_lower = FreqDist([t.lower() for t in tokens_no_punct])\n",
    "print(f'\"data\" aparece {freq_lower[\"data\"]} veces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845pqpc2uo5",
   "metadata": {},
   "source": [
    "d) Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1 (sin filtrar): Data science is the study of the extraction of knowledge from data.\n",
      "Frase 1 (filtrada): Data science study extraction knowledge data.\n",
      "\n",
      "Frase 2 (sin filtrar): It uses various techniques from many fields, including signal processing, mathematics, probability models, machine learning, computer programming, statistics, data engineering, pattern recognition and learning, visualization, uncertainty modeling, data warehousing, and high performance computing with the goal of extracting useful knowledge from the data.\n",
      "Frase 2 (filtrada): uses various techniques many fields, including signal processing, mathematics, probability models, machine learning, computer programming, statistics, data engineering, pattern recognition learning, visualization, uncertainty modeling, data warehousing, high performance computing goal extracting useful knowledge data.\n",
      "\n",
      "Frase 3 (sin filtrar): Data Science is not restricted to only big data, although the fact that data is scaling up makes big data an important aspect of data science.\n",
      "Frase 3 (filtrada): Data Science restricted big data, although fact data scaling makes big data important aspect data science.\n",
      "\n",
      "Frase 4 (sin filtrar): A practitioner of data science is called a data scientist.\n",
      "Frase 4 (filtrada): practitioner data science called data scientist.\n",
      "\n",
      "Frase 5 (sin filtrar): Data scientists solve complex data problems using various elements of mathematics, statistics and computer science, although expertise in these subjects are not required.\n",
      "Frase 5 (filtrada): Data scientists solve complex data problems using various elements mathematics, statistics computer science, although expertise subjects required.\n",
      "\n",
      "Frase 6 (sin filtrar): However, a data scientist is most likely to be an expert in only one or two of these disciplines, meaning that cross disciplinary teams can be a key component of data science.\n",
      "Frase 6 (filtrada): However, data scientist likely expert one two disciplines, meaning cross disciplinary teams key component data science.\n",
      "\n",
      "Frase 7 (sin filtrar): Good data scientists are able to apply their skills to achieve a broad spectrum of end results.\n",
      "Frase 7 (filtrada): Good data scientists able apply skills achieve broad spectrum end results.\n",
      "\n",
      "Frase 8 (sin filtrar): The skill-sets and competencies that data scientists employ vary widely.\n",
      "Frase 8 (filtrada): skill-sets competencies data scientists employ vary widely.\n",
      "\n",
      "Frase 9 (sin filtrar): Many data scientists practice and showcase their work on platforms such as Kaggle and Google Colab.\n",
      "Frase 9 (filtrada): Many data scientists practice showcase work platforms Kaggle Google Colab.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    tokens = word_tokenize(sent)\n",
    "    filtered = [t for t in tokens if t.lower() not in stop_words]\n",
    "    print(f'Frase {i+1} (sin filtrar): {detokenizer.detokenize(tokens)}')\n",
    "    print(f'Frase {i+1} (filtrada): {detokenizer.detokenize(filtered)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9meii2jrm8w",
   "metadata": {},
   "source": [
    "e) POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'NNP'),\n",
       " ('science', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('study', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('extraction', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('knowledge', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = pos_tag(word_tokenize(sentences[0]))\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5mvv8n2g25u",
   "metadata": {},
   "source": [
    "f) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data                 -> data\n",
      "science              -> scienc\n",
      "is                   -> is\n",
      "the                  -> the\n",
      "study                -> studi\n",
      "of                   -> of\n",
      "the                  -> the\n",
      "extraction           -> extract\n",
      "of                   -> of\n",
      "knowledge            -> knowledg\n",
      "from                 -> from\n",
      "data                 -> data\n",
      ".                    -> .\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "for token in word_tokenize(sentences[0]):\n",
    "    print(f'{token:20} -> {stemmer.stem(token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jkw54na5k8h",
   "metadata": {},
   "source": [
    "g) Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                Stem                 Lemma               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data                 data                 data                \n",
      "science              scienc               science             \n",
      "is                   is                   be                  \n",
      "the                  the                  the                 \n",
      "study                studi                study               \n",
      "of                   of                   of                  \n",
      "the                  the                  the                 \n",
      "extraction           extract              extraction          \n",
      "of                   of                   of                  \n",
      "knowledge            knowledg             knowledge           \n",
      "from                 from                 from                \n",
      "data                 data                 data                \n",
      ".                    .                    .                   \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def wn_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return 'n'\n",
    "\n",
    "print(f'{\"Token\":<20} {\"Stem\":<20} {\"Lemma\":<20}')\n",
    "for token, tag in pos_tag(word_tokenize(sentences[0])):\n",
    "    lemma = lemmatizer.lemmatize(token.lower(), wn_pos(tag))\n",
    "    print(f'{token:<20} {stemmer.stem(token):<20} {lemma:<20}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da1025",
   "metadata": {},
   "source": [
    "h) Entidades nombradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc4c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades encontradas (7):\n",
      "Data                           GPE\n",
      "Data                           PERSON\n",
      "Science                        ORGANIZATION\n",
      "Data                           GPE\n",
      "Good                           GPE\n",
      "Kaggle                         PERSON\n",
      "Google Colab                   PERSON\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    for chunk in ne_chunk(pos_tags, binary=False):\n",
    "        if isinstance(chunk, Tree):\n",
    "            entity = \" \".join(word for word, _ in chunk.leaves())\n",
    "            entity_type = chunk.label()\n",
    "            entities.append((entity, entity_type))\n",
    "\n",
    "print(f\"Entidades encontradas ({len(entities)}):\")\n",
    "for ent, label in entities:\n",
    "    print(f\"{ent:<30} {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Apartado 1.2: Texto en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las personas que se dedican a la ciencia de datos se les conoce como científico de datos. Se define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades:\n",
      "\n",
      "Recopilar, procesar y extraer valor de las diversas y extensas bases de datos.\n",
      "Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos.\n",
      "Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos.\n",
      "Los científicos de datos trabajan en todas las industrias y hacen frente a los grandes proyectos de datos en todos los niveles.\n",
      "El doctor en estadística Nathan Yau, precisó lo siguiente: el científico de datos es un estadístico que debería aprender interfaces de programación de aplicaciones (APIs), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado. 6\n",
      "\n",
      "En la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos. Entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “HCI” (Human-Computer Interaction). Además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso. 7\n",
      "\n",
      "Drew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(PATH_DATA / 'ciencia_datos.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j618d3vtmi",
   "metadata": {},
   "source": [
    "a) Tokenización texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Las personas que se dedican a la ciencia de datos se les conoce como científico de datos.',\n",
       " 'Se define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades:\\n\\nRecopilar, procesar y extraer valor de las diversas y extensas bases de datos.',\n",
       " 'Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos.',\n",
       " 'Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos.',\n",
       " 'Los científicos de datos trabajan en todas las industrias y hacen frente a los grandes proyectos de datos en todos los niveles.',\n",
       " 'El doctor en estadística Nathan Yau, precisó lo siguiente: el científico de datos es un estadístico que debería aprender interfaces de programación de aplicaciones (APIs), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado.',\n",
       " '6\\n\\nEn la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos.',\n",
       " 'Entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “HCI” (Human-Computer Interaction).',\n",
       " 'Además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso.',\n",
       " '7\\n\\nDrew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text, language='spanish')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544zsewm17o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: 383 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Las',\n",
       " 'personas',\n",
       " 'que',\n",
       " 'se',\n",
       " 'dedican',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ciencia',\n",
       " 'de',\n",
       " 'datos',\n",
       " 'se',\n",
       " 'les',\n",
       " 'conoce',\n",
       " 'como',\n",
       " 'científico',\n",
       " 'de',\n",
       " 'datos',\n",
       " '.',\n",
       " 'Se',\n",
       " 'define']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk = word_tokenize(text, language='spanish')\n",
    "print(f'word_tokenize: {len(tokens_nltk)} tokens')\n",
    "tokens_nltk[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0kezs8h6kn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split(): 336 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Las',\n",
       " 'personas',\n",
       " 'que',\n",
       " 'se',\n",
       " 'dedican',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ciencia',\n",
       " 'de',\n",
       " 'datos',\n",
       " 'se',\n",
       " 'les',\n",
       " 'conoce',\n",
       " 'como',\n",
       " 'científico',\n",
       " 'de',\n",
       " 'datos.',\n",
       " 'Se',\n",
       " 'define',\n",
       " 'al']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_split = text.split()\n",
    "print(f'split(): {len(tokens_split)} tokens')\n",
    "tokens_split[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ozwj3liueus",
   "metadata": {},
   "source": [
    "b) Eliminar puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opción 1 (list comprehension): 338 tokens\n",
      "Opción 2 (RegexpTokenizer): 338 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Las',\n",
       " 'personas',\n",
       " 'que',\n",
       " 'se',\n",
       " 'dedican',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ciencia',\n",
       " 'de',\n",
       " 'datos',\n",
       " 'se',\n",
       " 'les',\n",
       " 'conoce',\n",
       " 'como',\n",
       " 'científico',\n",
       " 'de',\n",
       " 'datos',\n",
       " 'Se',\n",
       " 'define',\n",
       " 'al']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opción 1: Filtrar con list comprehension\n",
    "punct = string.punctuation + '¿¡«»\"\"'\n",
    "tokens_no_punct = [t for t in tokens_nltk if t not in punct]\n",
    "print(f'Opción 1 (list comprehension): {len(tokens_no_punct)} tokens')\n",
    "\n",
    "# Opción 2: Usar RegexpTokenizer (tokeniza sin puntuación directamente)\n",
    "tokenizer_words = RegexpTokenizer(r'\\w+')\n",
    "tokens_no_punct_regex = tokenizer_words.tokenize(text)\n",
    "print(f'Opción 2 (RegexpTokenizer): {len(tokens_no_punct_regex)} tokens')\n",
    "\n",
    "tokens_no_punct[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12vtc54m51ga",
   "metadata": {},
   "source": [
    "c) Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con puntuación: [('de', 28), (',', 24), ('datos', 17), ('que', 12), ('y', 12)]\n",
      "Sin puntuación: [('de', 28), ('datos', 17), ('que', 12), ('y', 12), ('en', 12)]\n",
      "\"datos\" aparece 17 veces\n"
     ]
    }
   ],
   "source": [
    "freq_before = FreqDist(tokens_nltk)\n",
    "print('Con puntuación:', freq_before.most_common(5))\n",
    "\n",
    "freq_after = FreqDist(tokens_no_punct)\n",
    "print('Sin puntuación:', freq_after.most_common(5))\n",
    "\n",
    "freq_lower = FreqDist([t.lower() for t in tokens_no_punct])\n",
    "print(f'\"datos\" aparece {freq_lower[\"datos\"]} veces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ne8n6ve47bg",
   "metadata": {},
   "source": [
    "d) Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1 (sin filtrar): Las personas que se dedican a la ciencia de datos se les conoce como científico de datos.\n",
      "Frase 1 (filtrada): personas dedican ciencia datos conoce científico datos.\n",
      "\n",
      "Frase 2 (sin filtrar): Se define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades: Recopilar, procesar y extraer valor de las diversas y extensas bases de datos.\n",
      "Frase 2 (filtrada): define científico datos mezcla estadísticos, computólogos pensadores creativos, siguientes habilidades: Recopilar, procesar extraer valor diversas extensas bases datos.\n",
      "\n",
      "Frase 3 (sin filtrar): Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos.\n",
      "Frase 3 (filtrada): Imaginación comprender, visualizar comunicar conclusiones científicos datos.\n",
      "\n",
      "Frase 4 (sin filtrar): Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos.\n",
      "Frase 4 (filtrada): Capacidad crear soluciones basadas datos aumentan beneficios, reducen costos.\n",
      "\n",
      "Frase 5 (sin filtrar): Los científicos de datos trabajan en todas las industrias y hacen frente a los grandes proyectos de datos en todos los niveles.\n",
      "Frase 5 (filtrada): científicos datos trabajan todas industrias hacen frente grandes proyectos datos niveles.\n",
      "\n",
      "Frase 6 (sin filtrar): El doctor en estadística Nathan Yau, precisó lo siguiente: el científico de datos es un estadístico que debería aprender interfaces de programación de aplicaciones (APIs), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado.\n",
      "Frase 6 (filtrada): doctor estadística Nathan Yau, precisó siguiente: científico datos estadístico debería aprender interfaces programación aplicaciones (APIs), bases datos extracción datos; diseñador deberá aprender programar; computólogo deberá saber analizar encontrar datos significado.\n",
      "\n",
      "Frase 7 (sin filtrar): 6 En la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos.\n",
      "Frase 7 (filtrada): 6 tesis doctoral Benjamin Fry explicó proceso comprender mejor datos comenzaba serie números objetivo responder preguntas datos, cada fase proceso propone (adquirir, analizar, filtrar, extraer, representar, refinar interactuar), requiere diferentes enfoques especializados aporten mejor comprensión datos.\n",
      "\n",
      "Frase 8 (sin filtrar): Entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “ HCI ” (Human-Computer Interaction).\n",
      "Frase 8 (filtrada): enfoques menciona Fry: ingenieros sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas visualización información especialistas interacciones hombre-máquina, mejor conocidos siglas inglés “ HCI ” (Human-Computer Interaction).\n",
      "\n",
      "Frase 9 (sin filtrar): Además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso.\n",
      "Frase 9 (filtrada): Además, Fry afirmó contar diferentes enfoques especializados lejos resolver problema entendimiento datos, convierte parte problema, cada especialización conduce manera aislada problema camino hacia solución puede perder cada transición proceso.\n",
      "\n",
      "Frase 10 (sin filtrar): 7 Drew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos.\n",
      "Frase 10 (filtrada): 7 Drew Conway página web explica ayuda diagrama Venn, principales habilidades dan vida forma ciencia datos, así relaciones conjuntos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('spanish'))\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    tokens = word_tokenize(sent, language='spanish')\n",
    "    filtered = [t for t in tokens if t.lower() not in stop_words]\n",
    "    print(f'Frase {i+1} (sin filtrar): {detokenizer.detokenize(tokens)}')\n",
    "    print(f'Frase {i+1} (filtrada): {detokenizer.detokenize(filtered)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txb1hflf9om",
   "metadata": {},
   "source": [
    "e) POS tagging (aproximado con tagger inglés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Las', 'NNP'),\n",
       " ('personas', 'NN'),\n",
       " ('que', 'NN'),\n",
       " ('se', 'JJ'),\n",
       " ('dedican', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('la', 'NN'),\n",
       " ('ciencia', 'NN'),\n",
       " ('de', 'IN'),\n",
       " ('datos', 'FW'),\n",
       " ('se', 'JJ'),\n",
       " ('les', 'NNS'),\n",
       " ('conoce', 'VBP'),\n",
       " ('como', 'JJ'),\n",
       " ('científico', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK pos_tag está entrenado en inglés, por lo que no funciona bien para español\n",
    "pos_tag(word_tokenize(sentences[0], language='spanish'))[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w2443ueltpa",
   "metadata": {},
   "source": [
    "f) Stemming español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las                       -> las\n",
      "personas                  -> person\n",
      "que                       -> que\n",
      "se                        -> se\n",
      "dedican                   -> dedic\n",
      "a                         -> a\n",
      "la                        -> la\n",
      "ciencia                   -> cienci\n",
      "de                        -> de\n",
      "datos                     -> dat\n",
      "se                        -> se\n",
      "les                       -> les\n",
      "conoce                    -> conoc\n",
      "como                      -> com\n",
      "científico                -> cientif\n",
      "de                        -> de\n",
      "datos                     -> dat\n",
      ".                         -> .\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "for token in word_tokenize(sentences[0], language='spanish'):\n",
    "    print(f'{token:25} -> {stemmer.stem(token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yp5wi625lgl",
   "metadata": {},
   "source": [
    "g) Lematización español (con diccionario lemmatization_es.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario cargado: 491547 entradas\n"
     ]
    }
   ],
   "source": [
    "lemma_dict = {}\n",
    "with open(PATH_DATA / 'lemmatization_es.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            lemma, word = parts\n",
    "            lemma_dict[word.lower()] = lemma.lower()\n",
    "\n",
    "print(f'Diccionario cargado: {len(lemma_dict)} entradas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                     Stem                      Lemma                    \n",
      "Las                       las                       los                      \n",
      "personas                  person                    personar                 \n",
      "que                       que                       que                      \n",
      "se                        se                        se                       \n",
      "dedican                   dedic                     dedicar                  \n",
      "a                         a                         a                        \n",
      "la                        la                        lo                       \n",
      "ciencia                   cienci                    ciencia                  \n",
      "de                        de                        de                       \n",
      "datos                     dat                       dato                     \n",
      "se                        se                        se                       \n",
      "les                       les                       les                      \n",
      "conoce                    conoc                     conocer                  \n",
      "como                      com                       comer                    \n",
      "científico                cientif                   científico               \n",
      "de                        de                        de                       \n",
      "datos                     dat                       dato                     \n",
      ".                         .                         .                        \n"
     ]
    }
   ],
   "source": [
    "def lemmatize(word, lemma_dict):\n",
    "    return lemma_dict.get(word.lower(), word.lower())\n",
    "\n",
    "print(f'{\"Token\":<25} {\"Stem\":<25} {\"Lemma\":<25}')\n",
    "for token in word_tokenize(sentences[0], language='spanish'):\n",
    "    print(f'{token:<25} {stemmer.stem(token):<25} {lemmatize(token, lemma_dict):<25}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364cb3b",
   "metadata": {},
   "source": [
    "h) Entidades nombradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938c7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades encontradas (13):\n",
      "Las                            GPE\n",
      "Se                             GPE\n",
      "Imaginación                    GPE\n",
      "Capacidad                      GPE\n",
      "Los                            GPE\n",
      "El                             GPE\n",
      "Nathan Yau                     PERSON\n",
      "APIs                           ORGANIZATION\n",
      "Benjamin Fry                   PERSON\n",
      "Entre                          GPE\n",
      "Además                         GPE\n",
      "Fry                            PERSON\n",
      "Venn                           PERSON\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent, language='spanish')\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    for chunk in ne_chunk(pos_tags, binary=False):\n",
    "        if isinstance(chunk, Tree):\n",
    "            entity = \" \".join(word for word, _ in chunk.leaves())\n",
    "            entity_type = chunk.label()\n",
    "            entities.append((entity, entity_type))\n",
    "\n",
    "print(f\"Entidades encontradas ({len(entities)}):\")\n",
    "for ent, label in entities:\n",
    "    print(f\"{ent:<30} {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Análisis con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Apartado 2.1: Análisis básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"En 2026, la tecnología seguirá cambiando de forma notable cómo vivimos y cómo trabajamos cada día. Las tendencias de este año se centran en el impulso de la inteligencia artificial y la automatización, con actores clave como OpenAI y Google DeepMind. Asimismo, destacan una conectividad más potente y soluciones de sostenibilidad basadas en datos.\"\"\"\n",
    "\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lh84k9uba2",
   "metadata": {},
   "source": [
    "a) Token, POS y lema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En                   ADP        en\n",
      "2026                 NOUN       2026\n",
      ",                    PUNCT      ,\n",
      "la                   DET        el\n",
      "tecnología           NOUN       tecnología\n",
      "seguirá              VERB       seguir\n",
      "cambiando            VERB       cambiar\n",
      "de                   ADP        de\n",
      "forma                NOUN       forma\n",
      "notable              ADJ        notable\n",
      "cómo                 PRON       cómo\n",
      "vivimos              VERB       vivir\n",
      "y                    CCONJ      y\n",
      "cómo                 PRON       cómo\n",
      "trabajamos           VERB       trabajar\n",
      "cada                 DET        cada\n",
      "día                  NOUN       día\n",
      ".                    PUNCT      .\n",
      "Las                  DET        el\n",
      "tendencias           NOUN       tendencia\n",
      "de                   ADP        de\n",
      "este                 DET        este\n",
      "año                  NOUN       año\n",
      "se                   PRON       él\n",
      "centran              VERB       centrar\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:25]:\n",
    "    print(f'{token.text:<20} {token.pos_:<10} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7sht0sumcy8",
   "metadata": {},
   "source": [
    "b) Contar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: 26 de 60 tokens (43.3%)\n"
     ]
    }
   ],
   "source": [
    "stopwords_count = sum(1 for t in doc if t.is_stop)\n",
    "print(f'Stopwords: {stopwords_count} de {len(doc)} tokens ({stopwords_count/len(doc)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yqu3ejisk6",
   "metadata": {},
   "source": [
    "c) Adjetivos y nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjetivos: ['notable', 'artificial', 'potente', 'basadas']\n",
      "Nombres: ['2026', 'tecnología', 'forma', 'día', 'tendencias', 'año', 'impulso', 'inteligencia', 'automatización', 'actores', 'clave', 'conectividad', 'soluciones', 'sostenibilidad', 'datos']\n"
     ]
    }
   ],
   "source": [
    "adjetivos = [t.text for t in doc if t.pos_ == 'ADJ']\n",
    "nombres = [t.text for t in doc if t.pos_ == 'NOUN']\n",
    "print('Adjetivos:', adjetivos)\n",
    "print('Nombres:', nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jijwp0hug2m",
   "metadata": {},
   "source": [
    "d) POS con explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En              ADP        adposition\n",
      "2026            NOUN       noun\n",
      ",               PUNCT      punctuation\n",
      "la              DET        determiner\n",
      "seguirá         VERB       verb\n",
      "notable         ADJ        adjective\n",
      "cómo            PRON       pronoun\n",
      "y               CCONJ      coordinating conjunction\n",
      "como            SCONJ      subordinating conjunction\n",
      "OpenAI          PROPN      proper noun\n",
      "Asimismo        ADV        adverb\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "for token in doc:\n",
    "    if token.pos_ not in seen and not token.is_space:\n",
    "        print(f'{token.text:<15} {token.pos_:<10} {spacy.explain(token.pos_)}')\n",
    "        seen.add(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2m9kqmjf",
   "metadata": {},
   "source": [
    "e) Frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de frases: 3\n",
      "\n",
      "1. En 2026, la tecnología seguirá cambiando de forma notable cómo vivimos y cómo trabajamos cada día.\n",
      "2. Las tendencias de este año se centran en el impulso de la inteligencia artificial y la automatización, con actores clave como OpenAI y Google DeepMind.\n",
      "3. Asimismo, destacan una conectividad más potente y soluciones de sostenibilidad basadas en datos.\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(f'Número de frases: {len(sentences)}\\n')\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f'{i}. {sent.text.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbs25slsot9",
   "metadata": {},
   "source": [
    "f) Frases sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 2026 tecnología seguirá cambiando forma notable vivimos trabajamos\n",
      "2. tendencias año centran impulso inteligencia artificial automatización actores clave OpenAI Google DeepMind\n",
      "3. Asimismo destacan conectividad potente soluciones sostenibilidad basadas datos\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(sentences, 1):\n",
    "    filtered = [t.text for t in sent if not t.is_stop and not t.is_punct]\n",
    "    print(f'{i}. {\" \".join(filtered)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e3f02",
   "metadata": {},
   "source": [
    "g) Entidades nombradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c39314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las tendencias de este         MISC       Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "OpenAI                         MISC       Miscellaneous entities, e.g. events, nationalities, products or works of art\n",
      "Google DeepMind                MISC       Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:<30} {ent.label_:<10} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpl1g7xff39",
   "metadata": {},
   "source": [
    "h) Función de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Las tendencias tecnológicas más importantes marcarán el 2026.\n",
      "Procesado: tendencia tecnológico importante marcar 2026\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, nlp):\n",
    "    return ' '.join([\n",
    "        t.lemma_.lower()\n",
    "        for t in nlp(text)\n",
    "        if not t.is_stop and not t.is_punct\n",
    "    ])\n",
    "\n",
    "ejemplo = 'Las tendencias tecnológicas más importantes marcarán el 2026.'\n",
    "print(f'Original: {ejemplo}')\n",
    "print(f'Procesado: {preprocess_text(ejemplo, nlp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### Apartado 2.2: Matcher de SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATA / 'open_australia_1.txt', 'r', encoding='utf-8') as f:\n",
    "    text_aus1 = f.read()\n",
    "with open(PATH_DATA / 'open_australia_2.txt', 'r', encoding='utf-8') as f:\n",
    "    text_aus2 = f.read()\n",
    "\n",
    "all_texts = text_aus1 + '\\n' + text_aus2\n",
    "doc_all = nlp(all_texts)\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984yj4tekl8",
   "metadata": {},
   "source": [
    "a) Grand Slam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Slam (2):\n",
      "  Grand Slam (126, 128)\n",
      "  Grand Slam (179, 181)\n"
     ]
    }
   ],
   "source": [
    "matcher.add('GRAND_SLAM', [[{'LOWER': 'grand'}, {'LOWER': 'slam'}]])\n",
    "\n",
    "matches = matcher(doc_all)\n",
    "print(f'Grand Slam ({len(matches)}):')\n",
    "for _, start, end in matches:\n",
    "    print(f'  {doc_all[start:end].text} ({start}, {end})')\n",
    "matcher.remove('GRAND_SLAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as0aep4xx66",
   "metadata": {},
   "source": [
    "b) Lema \"volver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f78f65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formas de \"volver\" (1):\n",
      "  'volverá' en: ..., Rafael Nadal volverá a pelear por...\n"
     ]
    }
   ],
   "source": [
    "matcher.add('VOLVER', [[{'LEMMA': 'volver'}]])\n",
    "\n",
    "matches = matcher(doc_all)\n",
    "print(f'Formas de \"volver\" ({len(matches)}):')\n",
    "for _, start, end in matches:\n",
    "    ctx_start, ctx_end = max(0, start-3), min(len(doc_all), end+3)\n",
    "    print(f'  \\'{doc_all[start:end].text}\\' en: ...{doc_all[ctx_start:ctx_end].text}...')\n",
    "matcher.remove('VOLVER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d1d8b",
   "metadata": {},
   "source": [
    "c) Nombres y adjetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres y adjetivos (14):\n",
      "  sofocón final (6, 8)\n",
      "  primer punto (27, 29)\n",
      "  plena inferioridad (80, 82)\n",
      "  primeros parciales (85, 87)\n",
      "  sexta final (117, 119)\n",
      "  segundo título (133, 135)\n",
      "  vigesimoprimer major (140, 142)\n",
      "  achuchón sufrido (184, 186)\n",
      "  arreón final (208, 210)\n",
      "  sexta final (216, 218)\n",
      "  gran escenario (225, 227)\n",
      "  pista dura (250, 252)\n",
      "  solo paso (279, 281)\n",
      "  empate histórico (284, 286)\n"
     ]
    }
   ],
   "source": [
    "matcher.add('NOUN_ADJ', [[{'POS': 'NOUN'}, {'POS': 'ADJ'}]])\n",
    "matcher.add('ADJ_NOUN', [[{'POS': 'ADJ'}, {'POS': 'NOUN'}]])\n",
    "\n",
    "matches = matcher(doc_all)\n",
    "print(f'Nombres y adjetivos ({len(matches)}):')\n",
    "for _, start, end in matches[:15]:\n",
    "    print(f'  {doc_all[start:end].text} ({start}, {end})')\n",
    "\n",
    "matcher.remove('NOUN_ADJ')\n",
    "matcher.remove('ADJ_NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ntq67shnea",
   "metadata": {},
   "source": [
    "d) Nombre propio + verbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell-39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre propio + verbo (3):\n",
      "  Nadal sentó (18, 20)\n",
      "  Nadal disputará (99, 101)\n",
      "  Nadal volverá (173, 175)\n"
     ]
    }
   ],
   "source": [
    "matcher.add('PROPN_VERB', [[{'POS': 'PROPN'}, {'POS': 'VERB'}]])\n",
    "\n",
    "matches = matcher(doc_all)\n",
    "print(f'Nombre propio + verbo ({len(matches)}):')\n",
    "for _, start, end in matches:\n",
    "    print(f'  {doc_all[start:end].text} ({start}, {end})')\n",
    "matcher.remove('PROPN_VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Apartado 3.1: Normalización y detección de patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell-44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Este es un texto que tiene ejemplos de fechas. Hoy es 01/02/2026, esta es una fecha posterior al 13 de enero de 2026. ¿Nos gustaría estar ya a 5 de julio y empezar las vacaciones? Casi que mejor no, que el tiempo avance a su ritmo. El primer día de clase fue el 26-01-2025, y el último día será el 5 de mayo del 2025.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_fechas = '''Este es un texto que tiene ejemplos de fechas. Hoy es 01/02/2026, esta es una fecha posterior al 13 de enero de 2026. ¿Nos gustaría estar ya a 5 de julio y empezar las vacaciones? Casi que mejor no, que el tiempo avance a su ritmo. El primer día de clase fue el 26-01-2025, y el último día será el 5 de mayo del 2025.'''\n",
    "\n",
    "texto_fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1mgk04kd88h",
   "metadata": {},
   "source": [
    "a) Tokenizar por espacios en blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell-45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Este',\n",
       " 'es',\n",
       " 'un',\n",
       " 'texto',\n",
       " 'que',\n",
       " 'tiene',\n",
       " 'ejemplos',\n",
       " 'de',\n",
       " 'fechas.',\n",
       " 'Hoy',\n",
       " 'es',\n",
       " '01/02/2026,',\n",
       " 'esta',\n",
       " 'es',\n",
       " 'una']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_regex = re.split(r'\\s+', texto_fechas)\n",
    "print(f'Tokens: {len(tokens_regex)}')\n",
    "tokens_regex[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8jlvryvf6oq",
   "metadata": {},
   "source": [
    "b) Eliminar puntuación y palabras < 3 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens limpios: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Este',\n",
       " 'texto',\n",
       " 'que',\n",
       " 'tiene',\n",
       " 'ejemplos',\n",
       " 'fechas',\n",
       " 'Hoy',\n",
       " '01022026',\n",
       " 'esta',\n",
       " 'una',\n",
       " 'fecha',\n",
       " 'posterior',\n",
       " 'enero',\n",
       " '2026',\n",
       " 'Nos',\n",
       " 'gustaría',\n",
       " 'estar',\n",
       " 'julio',\n",
       " 'empezar',\n",
       " 'las',\n",
       " 'vacaciones',\n",
       " 'Casi',\n",
       " 'que',\n",
       " 'mejor',\n",
       " 'que',\n",
       " 'tiempo',\n",
       " 'avance',\n",
       " 'ritmo',\n",
       " 'primer',\n",
       " 'día',\n",
       " 'clase',\n",
       " 'fue',\n",
       " '26012025',\n",
       " 'último',\n",
       " 'día',\n",
       " 'será',\n",
       " 'mayo',\n",
       " 'del',\n",
       " '2025']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_clean = [re.sub(r'[^\\w]', '', t) for t in tokens_regex]\n",
    "tokens_clean = [t for t in tokens_clean if len(t) >= 3]\n",
    "print(f'Tokens limpios: {len(tokens_clean)}')\n",
    "tokens_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vsk8plr5vr",
   "metadata": {},
   "source": [
    "c) Detectar fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cell-47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DD/MM/AAAA: ['01/02/2026']\n",
      "DD-MM-AAAA: ['26-01-2025']\n",
      "DD de MM de AAAA: ['13 de enero de 2026']\n",
      "DD de MM: ['13 de ener', '5 de julio', '5 de mayo']\n"
     ]
    }
   ],
   "source": [
    "# DD/MM/AAAA\n",
    "pattern_slash = r'\\d{1,2}/\\d{2}/\\d{4}'\n",
    "# DD-MM-AAAA\n",
    "pattern_dash = r'\\d{1,2}-\\d{2}-\\d{4}'\n",
    "# DD de MM de AAAA\n",
    "pattern_full = r'\\d{1,2} de \\w+ de \\d{4}'\n",
    "# DD de MM (sin año)\n",
    "pattern_short = r'\\d{1,2} de \\w+(?! de \\d)'\n",
    "\n",
    "print('DD/MM/AAAA:', re.findall(pattern_slash, texto_fechas))\n",
    "print('DD-MM-AAAA:', re.findall(pattern_dash, texto_fechas))\n",
    "print('DD de MM de AAAA:', re.findall(pattern_full, texto_fechas))\n",
    "print('DD de MM:', re.findall(pattern_short, texto_fechas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell-48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las fechas encontradas (5):\n",
      "  01/02/2026\n",
      "  13 de enero de 2026\n",
      "  5 de julio\n",
      "  26-01-2025\n",
      "  5 de mayo del 2025\n"
     ]
    }
   ],
   "source": [
    "# Patrón combinado para todas las fechas\n",
    "pattern_all = r'\\d{1,2}[/-]\\d{2}[/-]\\d{4}|\\d{1,2} de \\w+(?: de(?:l)? \\d{4})?'\n",
    "fechas = re.findall(pattern_all, texto_fechas)\n",
    "print(f'Todas las fechas encontradas ({len(fechas)}):')\n",
    "for f in fechas:\n",
    "    print(f'  {f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-analisis-textual (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
